{
  "summary": {
    "num_model_tensors": 732,
    "top_keys": [
      "model",
      "model_version"
    ],
    "prefix_counts": {
      "module": 732
    }
  },
  "tensors": [
    {
      "key": "module.design_condition_embedder.condition_template_embedder.embedder.weight",
      "shape": [
        65,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_kv.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_kv.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_kv.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_kv.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight",
      "shape": [
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        4,
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        128,
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_kv.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_kv.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_kv.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_kv.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.weight",
      "shape": [
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        4,
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        128,
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_kv.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_kv.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_kv.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_kv.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_z.weight",
      "shape": [
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.linear_a_last.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.linear_a_last.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        4,
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        128,
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.linear_no_bias_cl.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.linear_no_bias_cm.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.linear_no_bias_d.weight",
      "shape": [
        16,
        3
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.linear_no_bias_f.weight",
      "shape": [
        128,
        385
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.linear_no_bias_invd.weight",
      "shape": [
        16,
        1
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.linear_no_bias_q.weight",
      "shape": [
        384,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.linear_no_bias_ref_charge.weight",
      "shape": [
        128,
        1
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.linear_no_bias_ref_pos.weight",
      "shape": [
        128,
        3
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.linear_no_bias_v.weight",
      "shape": [
        16,
        1
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.small_mlp.1.weight",
      "shape": [
        16,
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.small_mlp.3.weight",
      "shape": [
        16,
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.atom_attention_encoder.small_mlp.5.weight",
      "shape": [
        16,
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.input_map.bias",
      "shape": [
        449
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.design_condition_embedder.input_embedder.input_map.weight",
      "shape": [
        449,
        430
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_kv.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_kv.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_kv.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_kv.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight",
      "shape": [
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        4,
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        128,
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_kv.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_kv.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_kv.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_kv.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.weight",
      "shape": [
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        4,
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        128,
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_kv.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_kv.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_kv.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_kv.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_z.weight",
      "shape": [
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.linear_a_last.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.linear_a_last.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        4,
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        128,
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_kv.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_kv.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_kv.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_kv.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_z.weight",
      "shape": [
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.linear_a_last.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.linear_a_last.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        4,
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        128,
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.layernorm_q.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.linear_no_bias_a.weight",
      "shape": [
        128,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_decoder.linear_no_bias_out.weight",
      "shape": [
        3,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_kv.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_kv.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_kv.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_kv.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight",
      "shape": [
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        4,
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        128,
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_kv.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_kv.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_kv.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_kv.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.weight",
      "shape": [
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        4,
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        128,
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_kv.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_kv.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_kv.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_kv.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_z.weight",
      "shape": [
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.linear_a_last.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.linear_a_last.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        4,
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        128,
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.2.conditioned_transition_block.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_kv.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_kv.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_kv.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_kv.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_z.weight",
      "shape": [
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.linear_a_last.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.linear_a_last.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        4,
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        128,
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.linear_s.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.atom_transformer.diffusion_transformer.blocks.3.conditioned_transition_block.linear_s.weight",
      "shape": [
        128,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.layernorm_z.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.linear_no_bias_cl.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.linear_no_bias_cm.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.linear_no_bias_d.weight",
      "shape": [
        16,
        3
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.linear_no_bias_f.weight",
      "shape": [
        128,
        385
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.linear_no_bias_invd.weight",
      "shape": [
        16,
        1
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.linear_no_bias_q.weight",
      "shape": [
        768,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.linear_no_bias_r.weight",
      "shape": [
        128,
        3
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.linear_no_bias_ref_charge.weight",
      "shape": [
        128,
        1
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.linear_no_bias_ref_pos.weight",
      "shape": [
        128,
        3
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.linear_no_bias_s.weight",
      "shape": [
        128,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.linear_no_bias_v.weight",
      "shape": [
        16,
        1
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.linear_no_bias_z.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.small_mlp.1.weight",
      "shape": [
        16,
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.small_mlp.3.weight",
      "shape": [
        16,
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.atom_attention_encoder.small_mlp.5.weight",
      "shape": [
        16,
        16
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.fourier_embedding.b",
      "shape": [
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.fourier_embedding.w",
      "shape": [
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.layernorm_n.weight",
      "shape": [
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.layernorm_s.weight",
      "shape": [
        833
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.layernorm_z.weight",
      "shape": [
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.linear_no_bias_n.weight",
      "shape": [
        384,
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.linear_no_bias_s.weight",
      "shape": [
        384,
        833
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.linear_no_bias_z.weight",
      "shape": [
        128,
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.relpe.linear_no_bias.weight",
      "shape": [
        128,
        139
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.bias",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_s1.layernorm1.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias.weight",
      "shape": [
        384,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_a.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_s1.linear_no_bias_b.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.bias",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_s2.layernorm1.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias.weight",
      "shape": [
        384,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_a.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_s2.linear_no_bias_b.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_z1.layernorm1.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias.weight",
      "shape": [
        128,
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_a.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_z1.linear_no_bias_b.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.bias",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_z2.layernorm1.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias.weight",
      "shape": [
        128,
        256
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_a.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_conditioning.transition_z2.linear_no_bias_b.weight",
      "shape": [
        256,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.layernorm_z.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_a_last.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        768,
        1536
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.0.conditioned_transition_block.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.layernorm_z.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_a_last.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        768,
        1536
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.1.conditioned_transition_block.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.attention_pair_bias.layernorm_z.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.attention_pair_bias.linear_a_last.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.attention_pair_bias.linear_a_last.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        768,
        1536
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.conditioned_transition_block.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.10.conditioned_transition_block.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.attention_pair_bias.layernorm_z.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.attention_pair_bias.linear_a_last.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.attention_pair_bias.linear_a_last.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        768,
        1536
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.conditioned_transition_block.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.11.conditioned_transition_block.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.attention_pair_bias.layernorm_z.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.attention_pair_bias.linear_a_last.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.attention_pair_bias.linear_a_last.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        768,
        1536
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.conditioned_transition_block.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.12.conditioned_transition_block.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.attention_pair_bias.layernorm_z.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.attention_pair_bias.linear_a_last.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.attention_pair_bias.linear_a_last.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        768,
        1536
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.conditioned_transition_block.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.13.conditioned_transition_block.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.attention_pair_bias.layernorm_z.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.attention_pair_bias.linear_a_last.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.attention_pair_bias.linear_a_last.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        768,
        1536
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.conditioned_transition_block.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.14.conditioned_transition_block.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.attention_pair_bias.layernorm_z.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.attention_pair_bias.linear_a_last.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.attention_pair_bias.linear_a_last.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        768,
        1536
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.conditioned_transition_block.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.15.conditioned_transition_block.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.attention_pair_bias.layernorm_z.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.attention_pair_bias.linear_a_last.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.attention_pair_bias.linear_a_last.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        768,
        1536
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.conditioned_transition_block.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.2.conditioned_transition_block.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.attention_pair_bias.layernorm_z.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.attention_pair_bias.linear_a_last.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.attention_pair_bias.linear_a_last.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        768,
        1536
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.conditioned_transition_block.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.3.conditioned_transition_block.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.attention_pair_bias.layernorm_z.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.attention_pair_bias.linear_a_last.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.attention_pair_bias.linear_a_last.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        768,
        1536
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.conditioned_transition_block.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.4.conditioned_transition_block.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.attention_pair_bias.layernorm_z.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.attention_pair_bias.linear_a_last.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.attention_pair_bias.linear_a_last.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        768,
        1536
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.conditioned_transition_block.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.5.conditioned_transition_block.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.attention_pair_bias.layernorm_z.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.attention_pair_bias.linear_a_last.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.attention_pair_bias.linear_a_last.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        768,
        1536
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.conditioned_transition_block.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.6.conditioned_transition_block.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.attention_pair_bias.layernorm_z.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.attention_pair_bias.linear_a_last.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.attention_pair_bias.linear_a_last.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        768,
        1536
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.conditioned_transition_block.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.7.conditioned_transition_block.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.attention_pair_bias.layernorm_z.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.attention_pair_bias.linear_a_last.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.attention_pair_bias.linear_a_last.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        768,
        1536
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.conditioned_transition_block.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.8.conditioned_transition_block.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.attention_pair_bias.attention.linear_g.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.attention_pair_bias.attention.linear_k.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.attention_pair_bias.attention.linear_o.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.attention_pair_bias.attention.linear_q.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.attention_pair_bias.attention.linear_q.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.attention_pair_bias.attention.linear_v.weight",
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.attention_pair_bias.layernorm_a.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.attention_pair_bias.layernorm_a.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.attention_pair_bias.layernorm_a.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.attention_pair_bias.layernorm_a.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.attention_pair_bias.layernorm_z.weight",
      "shape": [
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.attention_pair_bias.linear_a_last.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.attention_pair_bias.linear_a_last.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.attention_pair_bias.linear_nobias_z.weight",
      "shape": [
        16,
        128
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.conditioned_transition_block.adaln.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.conditioned_transition_block.adaln.linear_nobias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.conditioned_transition_block.adaln.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.conditioned_transition_block.adaln.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.conditioned_transition_block.linear_nobias_a1.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.conditioned_transition_block.linear_nobias_a2.weight",
      "shape": [
        1536,
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.conditioned_transition_block.linear_nobias_b.weight",
      "shape": [
        768,
        1536
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.conditioned_transition_block.linear_s.bias",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.diffusion_transformer.blocks.9.conditioned_transition_block.linear_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.layernorm_a.weight",
      "shape": [
        768
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.layernorm_s.weight",
      "shape": [
        384
      ],
      "dtype": "torch.float32"
    },
    {
      "key": "module.diffusion_module.linear_no_bias_s.weight",
      "shape": [
        768,
        384
      ],
      "dtype": "torch.float32"
    }
  ]
}